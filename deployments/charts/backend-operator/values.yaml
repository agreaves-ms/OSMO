# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

## Global configuration values shared across all backend operators
##
global:
  ## Name to use for this deployment (if not provided, uses the Helm release name)
  ##
  name: null

  ## Base location for Osmo Docker images in the registry
  ##
  osmoImageLocation: nvcr.io/nvidia/osmo

  ## Docker image tag for Osmo backend operators
  ##
  osmoImageTag: latest

  ## Name of the Kubernetes secret containing Docker registry credentials
  ##
  imagePullSecret: null

  ## Node selector constraints for pod scheduling
  ##
  nodeSelector: {}
    # Add node selectors here as needed
    # kubernetes.io/arch: amd64
    # node-type: cpu
    # zone: us-west-2a

  ## Network configuration for backend operators
  ##
  network:
    ## Whether to restrict egress traffic from workflow pods
    ##
    restrictEgress: false

    ## Egress allowlist configuration for controlled external access
    ##
    allowlistEgress:
      ## Enable egress allowlist with squid proxy
      ##
      enabled: false

      ## Node selector for the squid proxy pod
      #
      nodeSelector: {}

      ## Kubernetes namespace where the squid proxy will be deployed
      ##
      proxyNamespace: osmo-squid-proxy

      ## Additional domains to allow through the proxy
      ##
      additionalAllowedDomains: []
      # - "example.com"
      # - "api.nvidia.com"

      ## Additional source IP addresses to allow
      ##
      additionalAllowedSourceIps: []
      # - "10.0.0.0/8"
      # - "192.168.1.0/24"

      ## Resource limits and requests for the squid proxy
      ##
      resources:
        requests:
          cpu: "2"
          memory: "4Gi"
        limits:
          memory: "4Gi"

      ## Sidecar containers for the proxy deployment
      ##
      sidecarContainers:
        - name: bridge-proxy
          image: envoyproxy/envoy:v1.34-latest
          command:
            - envoy
            - -c
            - /etc/envoy/envoy.yaml
          ports:
            - containerPort: 443
              name: bridge-https
            - containerPort: 11113
              name: admin
          volumeMounts:
            - name: envoy-config
              mountPath: /etc/envoy
            - name: envoy-certs
              mountPath: /certs

      ## Additional volumes for the proxy deployment
      ##
      additionalVolumes:
        - name: envoy-config
          configMap:
            name: bridge-envoy-config
        - name: envoy-certs
          secret:
            secretName: bridge-envoy-certs

      ## Host aliases for custom DNS resolution within proxy pods
      ##
      hostAliases: []

  ## OSMO service connection configuration
  ##
  ## URL of the OSMO service that backend operators will connect to
  ##
  serviceUrl: ""

  ## Username for authenticating with OSMO service (when using password authentication)
  ##
  accountUsername: ""

  ## Kubernetes secret containing the password for OSMO service authentication
  ##
  accountPasswordSecret: svc-osmo-admin

  ## Key within the password secret containing the actual password
  ##
  accountPasswordSecretKey: password

  ## Kubernetes secret containing the token for OSMO service authentication
  ##
  accountTokenSecret: agent-token

  ## Key within the token secret containing the actual token
  ##
  accountTokenSecretKey: token

  ## Authentication method to use with OSMO service (password or token)
  ##
  loginMethod: password

  ## Kubernetes namespace where backend operators will be deployed
  ##
  agentNamespace: osmo

  ## Name identifier for this backend
  ##
  backendName: default

  ## Kubernetes namespace where backend workloads will be scheduled
  ##
  backendNamespace: osmo-namespace

  ## Kubernetes namespace where backend cluster validation tests will be deployed
  ##
  backendTestNamespace: null

  ## Prefix for node condition labels (leave empty for no prefix)
  ##
  nodeConditionPrefix: ""


  ## Logging configuration for backend operators
  ##
  logs:
    ## Application log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    ##
    logLevel: DEBUG

    ## Kubernetes system log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    ##
    k8sLogLevel: WARNING

  ## Comma-separated list of namespaces to include in usage monitoring
  ##
  includeNamespaceUsage: 'osmo-staging,osmo-prod'

  ## Enable cluster-wide RBAC roles for backend operators
  ##
  enableClusterRoles: true

  ## Enable namespace-scoped RBAC roles for backend operators
  ##
  enableNonClusterRoles: true

  ## Tolerations for backend pod scheduling on tainted nodes
  ##
  tolerations:
  - key: "ops"
    operator: "Exists"
    effect: "NoSchedule"

  ## Priority class configuration for backend workloads to be used for scheduling
  ## only used if kaischeduler plugin is enabled
  priorityClasses:
    ## Enable creation of priority classes
    ##
    enabled: true

    ## List of priority classes to create
    ##
    classes:
      - name: osmo-high
        value: 125
        description: "Schedule as soon as possible. Non-preemptible."
      - name: osmo-normal
        value: 100
        description: "Standard priority. Non-preemptible."
      - name: osmo-low
        value: 50
        description: "Schedule last. Preemptible."

## Service account configuration shared by backend listener and worker pods
##
serviceAccount:
  ## Create the ServiceAccounts defined by this chart. Set to false to bind
  ## pre-provisioned workload identity ServiceAccounts
  ##
  create: true

  ## ServiceAccount name to use when creating or for an already provisioned
  ## ServiceAccount
  ##
  name: ""

  ## Extra annotations applied to ServiceAccounts (e.g.
  ## `azure.workload.identity/client-id`)
  ##
  annotations: {}

## Configuration for individual backend operators
##
services:
  ## Backend listener configuration
  ## Listens for job requests and manages backend cluster state
  ##
  backendListener:
    ## Enable automatic node label updates based on cluster state
    ##
    enableNodeLabelUpdate: false

    ## Docker image name for the backend listener
    ##
    imageName: backend-listener

    ## Kubernetes image pull policy for the backend listener
    ##
    imagePullPolicy: Always

    ## Kubernetes service name for the backend listener
    ##
    serviceName: osmo-backend-listener

    ## Init containers for backend listener
    ##
    initContainers: []

    ## Kubernetes service account name for the backend listener. Leave empty to
    ## use the chart default (`backend-listener`).
    ##
    serviceAccount: ""

    ## Maximum number of unacknowledged websocket messages
    ##
    max_unacked_messages: 100

    ## Time-to-live for pod cache entries (in seconds)
    ##
    podCacheTtl: 15

    ## Additional command line arguments to pass to the backend listener
    ##
    extraArgs: []
    # - "--debug"
    # - "--custom-flag=value"

    ## Additional environment variables for the backend listener container
    ##
    extraEnvs: []
    # - name: "CUSTOM_ENV_VAR"
    #   value: "custom-value"
    # - name: "SECRET_ENV_VAR"
    #   valueFrom:
    #     secretKeyRef:
    #       name: my-secret
    #       key: secret-key

    ## Additional pod annotations for the backend listener
    ##
    extraPodAnnotations: {}
    # custom.annotation/key: "value"
    # prometheus.io/scrape: "true"

    ## Additional pod labels for the backend listener
    ##
    extraPodLabels: {}
    # custom.label/key: "value"
    # environment: "production"

    ## Additional sidecar containers for the backend listener
    ##
    extraSidecarContainers: []
    # - name: custom-sidecar
    #   image: nginx:latest
    #   ports:
    #   - containerPort: 80
    #     name: http

    ## Node selector constraints for backend listener pod scheduling
    ##
    nodeSelector: {}
    # kubernetes.io/os: linux
    # node-type: operator

    ## Host aliases for custom DNS resolution within backend listener pods
    ##
    hostAliases: []
    # - ip: "192.168.1.100"
    #   hostnames:
    #   - "example.local"

    ## Volumes for backend listener
    ## Default includes progress files for liveness and startup probes
    ##
    volumes:
      - name: progress-files
        emptyDir: {}
    volumeMounts:
      - name: progress-files
        mountPath: /var/run/osmo

    ## Resource limits and requests for the backend listener container
    ##
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        memory: "2Gi"

    ## Kubernetes API client QPS (queries per second) setting.
    ##
    apiQps: 20

    ## Kubernetes API client burst setting.
    ##
    apiBurst: 30

  ## Backend worker configuration
  ## Executes jobs received from the backend listener
  ##
  backendWorker:
    ## Docker image name for the backend worker
    ##
    imageName: backend-worker

    ## Kubernetes image pull policy for the backend worker
    ##
    imagePullPolicy: Always

    ## Kubernetes service name for the backend worker
    ##
    serviceName: osmo-backend-worker

    ## Init containers for backend worker
    ##
    initContainers: []

    ## Kubernetes service account name for the backend worker. Leave empty to
    ## use the chart default (`backend-worker`).
    ##
    serviceAccount: ""

    ## How often to write progress during task processing loops
    ##
    progressIterFrequency: "15s"

    ## Additional command line arguments to pass to the backend worker
    ##
    extraArgs: []
    # - "--concurrency=10"
    # - "--timeout=3600"

    ## Additional environment variables for the backend worker container
    ##
    extraEnvs: []
    # - name: "WORKER_POOL_SIZE"
    #   value: "10"
    # - name: "DATABASE_URL"
    #   valueFrom:
    #     secretKeyRef:
    #       name: database-secret
    #       key: url

    ## Additional pod annotations for the backend worker
    ##
    extraPodAnnotations: {}
    # custom.annotation/key: "value"
    # prometheus.io/scrape: "true"

    ## Additional pod labels for the backend worker
    ##
    extraPodLabels: {}
    # custom.label/key: "value"
    # environment: "production"

    ## Additional sidecar containers for the backend worker
    ##
    extraSidecarContainers: []
    # - name: monitoring-sidecar
    #   image: prom/node-exporter:latest
    #   ports:
    #   - containerPort: 9100
    #     name: metrics

    ## Node selector constraints for backend worker pod scheduling
    ##
    nodeSelector: {}
    # kubernetes.io/os: linux
    # node-type: operator

    ## Host aliases for custom DNS resolution within backend worker pods
    ##
    hostAliases: []
    # - ip: "192.168.1.100"
    #   hostnames:
    #   - "example.local"

    ## Volumes for backend worker
    ## Default includes progress files for liveness and startup probes
    ##
    volumes:
      - name: progress-files
        emptyDir: {}
    volumeMounts:
      - name: progress-files
        mountPath: /var/run/osmo

    ## Resource limits and requests for the backend worker container
    ##
    resources:
      requests:
        cpu: "1"
        memory: "1Gi"
      limits:
        memory: "1Gi"


## Backend Test Runner CronJob Configuration
## This is used to setup backend tests
##
backendTestRunner:
  ## Enable backend test runner CronJob creation
  ##
  enabled: true

  ## CronJob scheduling configuration
  ##
  cronJob:
    ## Default schedule for running backend tests (cron format)
    ##
    schedule: "0 2 * * *"  # Daily at 2 AM

    ## Timezone for the CronJob schedule
    ##
    timezone: "UTC"

    ## Starting deadline in seconds for late CronJob executions
    ##
    startingDeadlineSeconds: 300

    ## Concurrency policy for CronJob execution
    ## Options: Allow, Forbid, Replace
    ##
    concurrencyPolicy: "Forbid"

    ## Whether to suspend CronJob execution
    ##
    suspend: false

    ## Number of successful job history entries to keep
    ##
    successfulJobsHistoryLimit: 1

    ## Number of failed job history entries to keep
    ##
    failedJobsHistoryLimit: 3

  ## Job template configuration
  ##
  jobTemplate:
    ## Number of parallel job executions
    ##
    parallelism: 1

    ## Number of successful completions required
    ##
    completions: 1

    ## Number of retries before marking job as failed
    ##
    backoffLimit: 2

    ## Maximum time in seconds for job execution
    ##
    activeDeadlineSeconds: 3600

    ## Time to live for completed jobs in seconds
    ##
    ttlSecondsAfterFinished: 86400

    ## Completion mode for the job
    ##
    completionMode: "NonIndexed"

    ## Whether to suspend job execution
    ##
    suspend: false

  ## Pod template configuration
  ##
  podTemplate:
    ## Container image configuration
    ##
    image:
      ## Docker image for backend test runner
      ##
      repository: "nvcr.io/nvidia/osmo/backend-test-runner"

      ## Image tag (if not specified, uses global.osmoImageTag)
      ##
      tag: ""

      ## Image pull policy
      ##
      pullPolicy: "Always"

    ## Init container configuration for config validation
    ##
    initContainer:
      ## Docker image for init container
      ##
      image: "ubuntu:22.04"

      ## Resource limits and requests for init container
      ##
      resources:
        requests:
          memory: "64Mi"
          cpu: "50m"
        limits:
          memory: "128Mi"
          cpu: "100m"

    ## Main container configuration
    ##
    container:
      ## Command arguments for the test runner
      ##
      args:
        - '--read_from_osmo'
        - 'false'
        - '--read_from_file'
        - '/test-config/test_config.json'
        - '--prefix'
        - 'osmo'


      ## Resource limits and requests for test runner container
      ##
      resources:
        requests:
          memory: "512Mi"
          cpu: "100m"
        limits:
          memory: "1Gi"
          cpu: "500m"

    ## Security context for pods
    ##
    securityContext:
      ## Run as non-root user
      ##
      runAsNonRoot: true

      ## User ID to run containers
      ##
      runAsUser: 1000

      ## Group ID to run containers
      ##
      runAsGroup: 1000

      ## File system group ID
      ##
      fsGroup: 1000

      ## Seccomp profile type
      ##
      seccompProfile:
        type: "RuntimeDefault"

    ## Container security context
    ##
    containerSecurityContext:
      ## Prevent privilege escalation
      ##
      allowPrivilegeEscalation: false

      ## Capabilities configuration
      ##
      capabilities:
        ## Capabilities to drop
        ##
        drop:
          - "ALL"
        ## Capabilities to add
        ##
        add:
          - "NET_BIND_SERVICE"

      ## Read-only root filesystem
      ##
      readOnlyRootFilesystem: true

    ## Kubernetes service account name for test runner pods. Leave empty to use
    ## the chart default (`test-runner`).
    ##
    serviceAccount: ""

    ## Whether to automount service account token
    ##
    automountServiceAccountToken: true

    ## Pod restart policy
    ##
    restartPolicy: "Never"

    ## Termination grace period in seconds
    ##
    terminationGracePeriodSeconds: 30

    ## DNS policy for pods
    ##
    dnsPolicy: "ClusterFirst"

    ## DNS configuration
    ##
    dnsConfig: {}

    ## Pod hostname template
    ##
    hostnameTemplate: "{{.BackendName}}-{{.TestName}}-runner"

    ## Pod subdomain
    ##
    subdomain: "backend-tests"

    ## Additional labels for pods
    ##
    labels:
      component: "backend-test-pod"
      pod-type: "test-runner"

    ## Additional annotations for pods
    ##
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"

    ## Node selector (inherits from global if not specified)
    ##
    nodeSelector: {}
    # kubernetes.io/os: linux
    # node-type: operator

    ## Tolerations (inherits from global if not specified)
    ##
    tolerations: []

    ## Host aliases for custom DNS resolution within backend test runner pods
    ##
    hostAliases: []
    # - ip: "192.168.1.100"
    #   hostnames:
    #   - "example.local"

    ## Affinity configuration
    ##
    affinity: {}

  ## ConfigMap configuration for test data
  ##
  configMap:
    ## ConfigMap name template
    ##
    nameTemplate: "{{.BackendName}}-{{.TestName}}-config"

    ## Default mode for ConfigMap files
    ##
    defaultMode: 0644

    ## Whether ConfigMap is optional
    ##
    optional: false

  ## Environment variables for test runner
  ##
  env:
    ## Additional environment variables
    ##
    additional: {}
    # CUSTOM_VAR: "custom_value"

  ## Volume configuration
  ##
  volumes:
    ## Test configuration volume name
    ##
    testConfigName: "test-config"

    ## Test configuration mount path
    ##
    testConfigMountPath: "/test-config"

  ## Labels to apply to all created resources
  ##
  labels:
    component: "backend-test"
    managed-by: "backend-operator"

  ## Annotations to apply to CronJob resources
  ##
  annotations:
    description: "Automated test runner for {{.BackendName}} - {{.TestName}}"
    scheduler: "backend-test-scheduler"

## Extra ConfigMaps
## Define additional ConfigMaps to be created alongside the chart
##
extraConfigMaps: {}
# Example:
# my-config:
#   data:
#     config.yaml: |
#       key: value
#       setting: enabled
#     script.sh: |
#       #!/bin/bash
#       echo "Hello World"
#   annotations:
#     description: "Custom configuration"
#   labels:
#     component: "custom"

## Configuration for sidecar containers
##
sidecars:
  ## OpenTelemetry (OTEL) configuration for observability
  ##
  OTEL:
    ## Enable OTEL collector sidecar for metrics and tracing
    ##
    enabled: false

    ## Docker image for OTEL collector
    ##
    image: otel/opentelemetry-collector-contrib:0.68.0

    ## ConfigMap name for OTEL collector configuration
    ##
    configName: otel-config

    ## Container ports configuration
    ##
    ports:
    - containerPort: 4000
      name: metrics

    ## Resource limits and requests for OTEL container
    ##
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        memory: "128Mi"
